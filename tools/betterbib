#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
from __future__ import print_function, unicode_literals

import argparse
import collections
# pylint: disable=import-error
import concurrent.futures
from io import open as io_open
import sys

# pylint: disable=import-self
import betterbib
from betterbib import pybtex_to_bibtex_string
from pybtex.database.input import bibtex
from tqdm import tqdm


def _main():
    args = _parse_cmd_arguments()

    data = bibtex.Parser().parse_file(args.infile)

    # Use an ordered dictionary to make sure that the entries are written out
    # sorted by their BibTeX key if demanded.
    od = collections.OrderedDict(
        sorted(data.entries.items())
        if args.sort_by_bibkey
        else data.entries.items()
        )

    if args.source == 'crossref':
        source = betterbib.Crossref(args.long_journal_name)
    else:
        assert args.source == 'dblp', 'Illegal source.'
        source = betterbib.Dblp()

    print()
    od, num_success = \
        _update_from_source(od, source, args.num_concurrent_requests)

    od = _adapt_doi_urls(od, args.doi_url_type)

    _write(od, args.outfile, args.delimeter_type)

    print('\n\nTotal number of entries: {}'.format(len(data.entries)))
    print('Found: {}'.format(num_success))
    return


def _adapt_doi_urls(od, doi_url_type):
    if doi_url_type == 'new':
        od = _update_doi_url(od, lambda doi: 'https://doi.org/' + doi)
    elif doi_url_type == 'short':
        def update_to_short_doi(doi):
            short_doi = betterbib.tools.get_short_doi(doi)
            if short_doi:
                return 'https://doi.org/' + short_doi
            return None
        od = _update_doi_url(od, update_to_short_doi)
    else:
        assert doi_url_type == 'unchanged'

    return od


def _update_doi_url(od, url_from_doi):
    for bib_id in od:
        if 'url' in od[bib_id].fields:
            doi = betterbib.tools.doi_from_url(od[bib_id].fields['url'])
            if doi:
                new_url = url_from_doi(doi)
                if new_url:
                    od[bib_id].fields['url'] = new_url
    return od


def _update_from_source(od, source, num_concurrent_requests):
    num_success = 0
    # pylint: disable=bad-continuation
    with concurrent.futures.ThreadPoolExecutor(
            max_workers=num_concurrent_requests
            ) as executor:
        responses = {
            executor.submit(source.find_unique, entry): (bib_id, entry)
            for bib_id, entry in od.items()
            }
        for future in tqdm(
                concurrent.futures.as_completed(responses),
                total=len(responses)
                ):
            bib_id, entry = responses[future]
            data = None
            try:
                data = future.result()
            except (betterbib.errors.NotFoundError,
                    betterbib.errors.UniqueError):
                pass
            except betterbib.errors.HttpError as e:
                print(e.args[0])
            else:
                num_success += 1

            od[bib_id] = data if data else entry

    return od, num_success


def _write(od, filename, delimeter_type):
    # Open output file for writing.
    out = io_open(filename, mode='w', encoding='utf-8')

    # Write header to the output file.
    out.write(
        '%%comment{This file was created with betterbib v%s.}\n\n' %
        betterbib.__version__
        )

    # Create the dictionary only once
    dictionary = betterbib.create_dict()

    # write the data out sequentially to respect ordering
    for bib_id, d in od.items():
        bracket_delimeters = delimeter_type == 'curly'
        a = pybtex_to_bibtex_string(
            d, bib_id, bracket_delimeters=bracket_delimeters,
            dictionary=dictionary
            )
        out.write(a + '\n\n')

    out.close()
    return


def _parse_cmd_arguments():
    parser = argparse.ArgumentParser(
        description='Improve BibTeX files '
        'with information from online sources.'
        )
    parser.add_argument(
        'infile',
        type=str,
        help='input BibTeX file'
        )
    parser.add_argument(
        '-v', '--version',
        help='display version information',
        action='version',
        version='%(prog)s {}, Python {}'.format(
            betterbib.__version__, sys.version
            )
        )
    parser.add_argument(
        'outfile',
        type=str,
        help='output BibTeX file'
        )
    parser.add_argument(
        '-s', '--source',
        choices=[
            'crossref',
            'dblp',
            ],
        default='crossref',
        help='data source (default: crossref)',
        )
    parser.add_argument(
        '-l', '--long-journal-name',
        action='store_true',
        help='prefer long journal names (default: false)'
        )
    parser.add_argument(
        '-b', '--sort-by-bibkey',
        action='store_true',
        help='sort entries by BibTeX key (default: false)'
        )
    parser.add_argument(
        '-d', '--delimeter-type',
        choices=[
            'curly',
            'quote',
            ],
        default='curly',
        help=(
            'which delimeters to use in the output file '
            '(default: curly brackets)'
            ),
        )
    parser.add_argument(
        '-c', '--num-concurrent-requests',
        type=int,
        default=10,
        metavar='N',
        help='number of concurrent HTTPS requests (default: 10)',
        )
    parser.add_argument(
        '-u', '--doi-url-type',
        choices=[
            'unchanged',
            'new',
            'short'
            ],
        default='new',
        help=(
            'DOI URL (new: https://doi.org/<DOI> (default), '
            'short: https://doi.org/abcde)'
            ),
        )
    return parser.parse_args()


if __name__ == '__main__':
    _main()
